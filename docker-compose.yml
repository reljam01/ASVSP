services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - 9870:9870   # NameNode web UI port
      - 9000:9000   # NameNode IPC port
    volumes:
      - namenode-data:/hadoop/dfs/name
    networks:
      - hadoopnet

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - 9864:9864   # DataNode 1 web UI port
    volumes:
      - datanode1-data:/hadoop/dfs/data
    depends_on:
      - namenode
    networks:
      - hadoopnet

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    ports:
      - 9865:9864   # DataNode 2 web UI port
    volumes:
      - datanode2-data:/hadoop/dfs/data
    depends_on:
      - namenode
    networks:
      - hadoopnet

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    ports:
      - "8088:8088"  # ResourceManager web UI port
    depends_on:
      - namenode
    networks:
      - hadoopnet

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
    depends_on:
      - resourcemanager
    networks:
      - hadoopnet

  airflow-webserver:
    image: apache/airflow:2.6.1
    container_name: airflow-webserver
    secrets:
      - airflow_fernet_key
    depends_on:
      - namenode
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=/run/secrets/airflow_fernet_key
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:airflow_password@postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./batch_loader.sh:/opt/airflow/scripts/batch_loader.sh
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command:
      - bash
      - -c
      - |
        export AIRFLOW__CORE__FERNET_KEY=$(cat /run/secrets/airflow_fernet_key) && \
        airflow db init && \
        airflow users create \
          --username admin \
          --password admin \
          --firstname admin \
          --lastname admin \
          --role Admin \
          --email admin@dummy.com && \
        airflow webserver
    networks:
      - hadoopnet

  airflow-scheduler:
    image: apache/airflow:2.6.1
    container_name: airflow-scheduler
    user: root
    secrets:
      - airflow_fernet_key
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=/run/secrets/airflow_fernet_key
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow_user:airflow_password@postgres:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./batch_loader.sh:/opt/airflow/scripts/batch_loader.sh
      - ./batch:/opt/airflow/batch
      - /var/run/docker.sock:/var/run/docker.sock
    command:
      - bash
      - -c
      - |
        export AIRFLOW__CORE__FERNET_KEY=$(cat /run/secrets/airflow_fernet_key) && \
        airflow db init && \
        airflow scheduler && \
        sleep 30 && \
        airflow dags trigger run_load_script_on_airflow_start
    networks:
      - hadoopnet

  # PostgreSQL Database Service for Airflow
  postgres:
    image: postgres:13
    container_name: airflow-postgres
    environment:
      - POSTGRES_USER=airflow_user
      - POSTGRES_PASSWORD=airflow_password
      - POSTGRES_DB=airflow
    volumes:
     - postgres_data:/var/lib/postgresql/data
    networks:
      - hadoopnet

volumes:
  namenode-data:
  datanode1-data:
  datanode2-data:
  postgres_data:

networks:
  hadoopnet:
    driver: bridge

secrets:
  airflow_fernet_key:
    file: ./secrets/fernet_key.txt
